{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_DF(file_path):\n",
    "    file_rdd = sc.textFile(file_path)\n",
    "    header = file_rdd.first() # Obtenemos el header\n",
    "    DF = file_rdd.filter(lambda line: line != header)\\\n",
    "                            .map(lambda line: line.replace('/t', ' '))\\\n",
    "                            .map(lambda line: create_row(header)(*line.split(',')))\\\n",
    "                            .toDF()\n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_row(header): \n",
    "    return Row (*tuple(header.split(',')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_frame = get_DF('fatalities_country.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country_id: string (nullable = true)\n",
      " |-- fatalities: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_frame.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_frame.registerTempTable('africa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_country_df(country):\n",
    "    return sqlContext.sql('select country_id, fatalities from africa where country_id = %s' % country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSERT INTO public.fatalities VALUES (1,765,1334);\n",
      "INSERT INTO public.fatalities VALUES (2,183,16);\n",
      "INSERT INTO public.fatalities VALUES (3,1204,1265);\n",
      "INSERT INTO public.fatalities VALUES (4,8228,74883);\n",
      "INSERT INTO public.fatalities VALUES (5,760,3101);\n",
      "INSERT INTO public.fatalities VALUES (6,2120,45986);\n",
      "INSERT INTO public.fatalities VALUES (7,613,7585);\n",
      "INSERT INTO public.fatalities VALUES (8,17489,23875);\n",
      "INSERT INTO public.fatalities VALUES (9,1558,3951);\n",
      "INSERT INTO public.fatalities VALUES (10,370,497);\n",
      "INSERT INTO public.fatalities VALUES (11,8541,50157);\n",
      "INSERT INTO public.fatalities VALUES (12,503,3329);\n",
      "INSERT INTO public.fatalities VALUES (13,501,258);\n",
      "INSERT INTO public.fatalities VALUES (14,115,14);\n",
      "INSERT INTO public.fatalities VALUES (15,367,400);\n",
      "INSERT INTO public.fatalities VALUES (16,3698,12939);\n",
      "INSERT INTO public.fatalities VALUES (17,1004,231);\n",
      "INSERT INTO public.fatalities VALUES (18,932,1382);\n",
      "INSERT INTO public.fatalities VALUES (19,273,63);\n",
      "INSERT INTO public.fatalities VALUES (20,221,130);\n",
      "INSERT INTO public.fatalities VALUES (21,5363,379);\n",
      "INSERT INTO public.fatalities VALUES (22,4602,34);\n",
      "INSERT INTO public.fatalities VALUES (23,396,79339);\n",
      "INSERT INTO public.fatalities VALUES (24,4270,12616);\n",
      "INSERT INTO public.fatalities VALUES (25,125,109);\n",
      "INSERT INTO public.fatalities VALUES (26,640,730);\n",
      "INSERT INTO public.fatalities VALUES (27,2888,8862);\n",
      "INSERT INTO public.fatalities VALUES (28,903,249);\n",
      "INSERT INTO public.fatalities VALUES (29,395,132);\n",
      "INSERT INTO public.fatalities VALUES (30,134,20);\n",
      "INSERT INTO public.fatalities VALUES (31,2424,15636);\n",
      "INSERT INTO public.fatalities VALUES (32,209,956);\n",
      "INSERT INTO public.fatalities VALUES (33,1149,2778);\n",
      "INSERT INTO public.fatalities VALUES (34,643,275);\n",
      "INSERT INTO public.fatalities VALUES (35,3025,143628);\n",
      "INSERT INTO public.fatalities VALUES (36,614,6403);\n",
      "INSERT INTO public.fatalities VALUES (37,39,33);\n",
      "INSERT INTO public.fatalities VALUES (38,6917,8833);\n",
      "INSERT INTO public.fatalities VALUES (39,7421,1304);\n",
      "INSERT INTO public.fatalities VALUES (40,110,178);\n",
      "INSERT INTO public.fatalities VALUES (41,100,90);\n",
      "INSERT INTO public.fatalities VALUES (42,607,333);\n",
      "INSERT INTO public.fatalities VALUES (43,1949,581);\n",
      "INSERT INTO public.fatalities VALUES (44,4611,14767);\n",
      "INSERT INTO public.fatalities VALUES (45,3497,21273);\n",
      "INSERT INTO public.fatalities VALUES (46,5041,9129);\n",
      "INSERT INTO public.fatalities VALUES (47,436,1715);\n",
      "INSERT INTO public.fatalities VALUES (48,9825,73897);\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,49):\n",
    "    data_frame = get_country_df(i)\n",
    "    total_fatalities = data_frame.map(lambda x: x.fatalities).reduce(lambda a,b: int(a)+int(b))\n",
    "    total_events = data_frame.count()\n",
    "    print \"INSERT INTO public.fatalities VALUES (%s,%s,%s);\" % (i, total_events, total_fatalities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_frame = get_DF('info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_frame.registerTempTable('africa_countries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+\n",
      "|       tableName|isTemporary|\n",
      "+----------------+-----------+\n",
      "|africa_countries|       true|\n",
      "|          africa|       true|\n",
      "+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dateutil.parser import parse\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.mllib.clustering import KMeans, StreamingKMeans, GaussianMixture, PowerIterationClustering\n",
    "from pyspark.mllib.feature import StandardScaler\n",
    "import numpy as np\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sqlContext.sql('select country_id, event_date, fatalities, latitud, longitud from africa_countries')\\\n",
    "        .map(lambda x: Vectors.dense(float(x.latitud),\\\n",
    "                                     float(x.longitud),\n",
    "                                     int((datetime.datetime.now()-parse(x.event_date)).seconds),\\\n",
    "                                     int(x.fatalities)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = rdd.map(lambda x: [x[0],x[1],x[2]])\n",
    "fatalities = rdd.map(lambda x: x[3])\n",
    "scaler = StandardScaler(withMean=True, withStd=True).fit(features)\n",
    "normalizedData = scaler.transform(features)\n",
    "nClusters = 10\n",
    "model = KMeans().train(normalizedData, nClusters, maxIterations=100,\\\n",
    "                        initializationMode=\"k-means||\")\n",
    "labels = model.predict(normalizedData)\n",
    "labels.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 2, 2, 2]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.take(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "widgets": {
   "state": {},
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
